<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"stellarkey.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"P1TCRNQBR7","apiKey":"501e087ded257acc9dc481944c14c248","indexName":"云端","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>


  <meta name="description" content="《数学之美》是吴军的作品。主要包括自然语言处理，信息学和搜索算法等内容。">
<meta property="og:type" content="article">
<meta property="og:title" content="阅《数学之美》">
<meta property="og:url" content="https://stellarkey.github.io/Beauty-of-Mathmatics/index.html">
<meta property="og:site_name" content="思维之海">
<meta property="og:description" content="《数学之美》是吴军的作品。主要包括自然语言处理，信息学和搜索算法等内容。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/LiivAdu.png">
<meta property="og:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/uVGlpvL.png">
<meta property="og:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/U57DH6s.png">
<meta property="og:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/jW212Sg.png">
<meta property="og:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/Logistic-curve.svg">
<meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*eEKb2RxREV6-MtLz2DNWFQ.gif">
<meta property="og:image" content="https://cdn-images-1.medium.com/max/800/1*n6sJ4yZQzwKL9wnF5wnVNg.png">
<meta property="og:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/lMn0aAi.jpg">
<meta property="article:published_time" content="2018-12-23T23:03:08.000Z">
<meta property="article:modified_time" content="2021-01-04T15:53:26.859Z">
<meta property="article:author" content="工云">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="幻想学">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://stellarkey.github.io/Beauty-of-Mathmatics/LiivAdu.png">

<link rel="canonical" href="https://stellarkey.github.io/Beauty-of-Mathmatics/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>阅《数学之美》 | 思维之海</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script async defer data-website-id="d07df043-7587-49c8-bc01-ac5ebfdafee5" src="https://stellarkey.asia/umami.js"></script>
<link rel="alternate" href="/atom.xml" title="思维之海" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">思维之海</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">——在云端，寻找我的星匙。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fas fa-anchor fa-fw"></i> </a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fas fa-th fa-fw"></i> </a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fas fa-tags fa-fw"></i> </a>

  </li>
        <li class="menu-item menu-item-link">

    <a href="/link/" rel="section"><i class="fas fa-link fa-fw"></i> </a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fas fa-user fa-fw"></i> </a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

    
    

    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://stellarkey.github.io/Beauty-of-Mathmatics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/stellarkey.gif">
      <meta itemprop="name" content="工云">
      <meta itemprop="description" content="Never stop thinking.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="思维之海">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          阅《数学之美》
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learn/" itemprop="url" rel="index"><span itemprop="name">Learn</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>《<a target="_blank" rel="noopener external nofollow noreferrer" href="https://book.douban.com/subject/10750155/">数学之美</a>》是<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.aminocapital.com/team/wu-jun-bo-shi">吴军</a>的作品。<br>主要包括自然语言处理，信息学和搜索算法等内容。 <span id="more"></span></p>
</blockquote>
<p>这是Vel的读书笔记，收获+思考。</p>
<h1 id="自然语言"><a href="#自然语言" class="headerlink" title="自然语言"></a>自然语言</h1><h2 id="起源时期"><a href="#起源时期" class="headerlink" title="起源时期"></a>起源时期</h2><p>一个<strong>信息的原始传播模型</strong>：</p>
<script type="math/tex; mode=display">
【发送】信息 \rightarrow编码\rightarrow信息（信道）\rightarrow解码\rightarrow信息【接收】</script><p>等价地用幻想学语言表述为：</p>
<script type="math/tex; mode=display">
现实 \rightarrow幻想 \rightarrow联想\rightarrow信息\rightarrow逆联想\rightarrow幻想\rightarrow现实</script><p>对于$“现实\rightarrow幻想 \rightarrow联想\rightarrow信息<br>”$部分。</p>
<blockquote>
<p>在人类发展以后，逐渐出现了庞大的信息载体：<strong>象形文字</strong>。</p>
<p>然而象形文字太过于唯象（就是说，任何事物都用一个初等联想来表征），导致人类需要的记忆量很大，因此必须通过编码（联想）加以简化。</p>
<p>在联想过程中，又产生了复杂联想，如多方联想（<strong>聚类</strong>，一词多义），从而产生特征群。</p>
</blockquote>
<p>但当我们只研究$“联想\rightarrow信息\rightarrow逆联想”$部分时。可以得出另一个结论：</p>
<blockquote>
<p><strong>翻译</strong>的可行性，是因为<strong>不同的文字系统在记录信息上的能力是等价的</strong>。</p>
</blockquote>
<p>对比幻想学中对翻译的定义：</p>
<blockquote>
<p>翻译：根据任意给定的想界，对特定幻想的可想特征联想称为该想界下对该幻想的翻译。</p>
</blockquote>
<p>都可以发现，翻译的主要局限在于<strong>想界</strong>。也就是说，想界决定了文字的复杂度上限，而翻译并不依赖于特定的文字系统（联想群）。</p>
<p>接着书中介绍了<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E7%BE%85%E5%A1%9E%E5%A1%94%E7%9F%B3%E7%A2%91">罗塞塔石碑</a>。（同名语言学习软件：<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.rosettastone.cn/">Rosettastone</a>）</p>
<ul>
<li>信息的冗余提高信息的安全性。</li>
<li>语言的数据（语料）对翻译至关重要。</li>
</ul>
<p>随着资源的丰富化，人类语言开始出现<strong>数字</strong>。</p>
<p>进一步，出现了<strong>进位制</strong>。（普遍10进制。玛雅文明使用了20进制）</p>
<ul>
<li>中国：个十百千万亿兆（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/ce123_zhouwei/article/details/6971544">大端</a>）</li>
<li>罗马：$IVXM$（大端相减，小端相加）</li>
<li>印度：0123456789（隐式进制单位）</li>
</ul>
<p>自然语言从此与数字分道扬镳。</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E6%A5%94%E5%BD%A2%E6%96%87%E5%AD%97">楔形文字</a>是最早的拼音文字。</p>
<blockquote>
<p>若把中文中的拼音当作字母，则它也是一种（二维）拼音文字。<br>可以认为历史发展中，中文字体从抽象的图逐渐规整化，从而表意$\rightarrow$表音。</p>
</blockquote>
<p>从楔形文字中传播简化出的22个字母后来演变成欧亚非大陆语言体系的主体。</p>
<hr>
<p>词是有限而封闭的集合，语言则是无限而开放的集合。</p>
<blockquote>
<p>任何语言都有语法规则覆盖不到的地方。<br>从而引出了语言和语法之争。</p>
</blockquote>
<h2 id="转折时期"><a href="#转折时期" class="headerlink" title="转折时期"></a>转折时期</h2><ul>
<li>计算机能否处理自然语言？Yes。</li>
<li>处理方法是否与人一样？Yes。</li>
</ul>
<p>早期：</p>
<blockquote>
<p>1950-170。用电脑模拟人脑。成果几乎为0。</p>
</blockquote>
<p>中期：</p>
<blockquote>
<p>1970-21世纪。基于数学模型+统计方法。实质性突破。</p>
</blockquote>
<p>核心结论：<strong>计算机并不需要拥有类似人类的智能才能完成翻译</strong>。（<strong><font color=red>唯象派</font></strong>）</p>
<hr>
<p>早期的研究如下：</p>
<blockquote>
<ul>
<li>应用层：语音识别，机器翻译，自动问答</li>
<li>认知层：自然语言处理</li>
<li><strong>基础层</strong>：句法分析，语义分析</li>
</ul>
</blockquote>
<p>但是，由于基础分析的复杂性，研究迟迟没有进展。</p>
<p>可以参考：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E6%8A%BD%E8%B1%A1%E8%AA%9E%E6%B3%95%E6%A8%B9">抽象语法树</a>，以及<a target="_blank" rel="noopener external nofollow noreferrer" href="http://huang-jerryc.com/2016/03/15/%E4%BD%95%E4%B8%BA%E8%AF%AD%E6%B3%95%E6%A0%91/">何为语法树</a>。自然语言的语法树所需要定义的文法规则过多。</p>
<blockquote>
<p>实际上，就是因为幻想和幻想之间的联想<strong>被一个一个地处理</strong>，计算机并没有学会通用联想规则。</p>
<p>其次，由于文本的<strong>上下文相关</strong>，更加大难度。这类特征联想也没有被掌握。</p>
<ul>
<li>算法复杂度——上下文无关算法：$O(n^2)$；上下文相关算法：$O(n^6)$。</li>
</ul>
</blockquote>
<hr>
<p>基于统计的自然语言处理方法，在数学模型上和通信是相通的（甚至相同的）。</p>
<h2 id="统计模型"><a href="#统计模型" class="headerlink" title="统计模型"></a>统计模型</h2><h3 id="通用模型"><a href="#通用模型" class="headerlink" title="通用模型"></a>通用模型</h3><p>初衷：<strong>语音识别</strong>，判定一个文字序列是否可被理解并有意义。</p>
<blockquote>
<p>比如在上一章的例子中：</p>
<ul>
<li>美联储主席本·伯南克昨天告诉媒体7 000 亿美元的救助资金将借给上百家银行、保险公司和汽车公司。</li>
</ul>
<p>这句话就很通顺【语法】，意思【词义】也很明白。</p>
<p>如果改变一些词的顺序，或者替换掉一些词，将这句话变成：</p>
<ul>
<li>本·伯南克美联储主席昨天7 000 亿美元的救助资金告诉媒体将借给银行、保险公司和汽车公司上百家。</li>
</ul>
<p>意思就含混了，虽然多少还能猜到一点。【词义】</p>
<p>但是如果再换成：</p>
<ul>
<li>联主美储席本·伯诉体南将借天的救克告媒昨助资金70 元亿00 美给上百百百家银保行、汽车险公司公司和。</li>
</ul>
<p>基本上读者就不知所云了。【模糊】</p>
<p>上面的例子体现了一个转变过程：<strong>语法——词义——模糊</strong>。</p>
</blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/%E8%B4%BE%E9%87%8C%E5%B0%BC%E5%85%8B">贾里尼克</a>提出：（拓展：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/BCJR%E7%AE%97%E6%B3%95">BCJR算法</a>）</p>
<blockquote>
<p><strong>一个句子是否合理，就看看它出现的可能性大小如何</strong>。至于可能性就用概率来衡量。<br>第一个句子出现的概率大致是10-20，第二个句子出现的概率是10-25 次方，第三个句子出现的概率是10-70。因此，第一个最有可能，它的可能是第二个句子的10 万倍，是第三个句子的一百亿亿亿亿亿亿倍。</p>
<p>（注：为避免值太小，可以使用对数概率）</p>
</blockquote>
<p>引入<strong>马尔可夫假设</strong>：（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE">马尔可夫</a>）</p>
<ul>
<li>假设任意一个词出现的概率<strong>只同它前面的词</strong>有关。（<strong>前驱</strong>，联想链表）</li>
</ul>
<img data-src="/Beauty-of-Mathmatics/LiivAdu.png" class="" title="二元模型">
<blockquote>
<p>条件概率公式为</p>
<script type="math/tex; mode=display">
P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)…P(w_n|w_1,w_2,…,w_{n-1})</script><p>利用马尔可夫假设：</p>
<script type="math/tex; mode=display">
P(w_k|w_1,w_2,…,w_{k-1})=P(w_k|w_{k-1})</script><p>于是得到</p>
</blockquote>
<script type="math/tex; mode=display">
P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_n|w_{n-1})</script><p>该公式对应着<strong><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/N%E5%85%83%E8%AF%AD%E6%B3%95">二元模型</a></strong>（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/29824784">More</a>）。</p>
<blockquote>
<p>当所考虑的前驱不止一个时（一般为有限个），称为<strong>n元语法模型</strong>。</p>
</blockquote>
<img data-src="/Beauty-of-Mathmatics/uVGlpvL.png" class="" title="n元语法模型">
<blockquote>
<p>对应的简化公式为：（以三元为例）</p>
<script type="math/tex; mode=display">
P(w_1,w_2,…,w_n)=P(w_1)P(w_2)P(w_3|w_2,w_1)…P(w_n|w_{n-1},w_{n-2})</script></blockquote>
<p>这样的模型之所以在自然语言中很成功，是因为大多数自然语言都具有<strong><font color=red>局部性</font></strong>。（人类的联想大多是初等的）</p>
<blockquote>
<p>由于现实中，联想总是相对稀少，统计模型的成功也就不难理解了。事实上，越是重复度高，统计模型就越有用武之地。（大数定律）</p>
<blockquote>
<p>比如，翻译一个几乎很少被使用的生僻词汇，可能就需要非统计学的手段。（频率将不可靠）</p>
<p>对于跨越上下文的”长程依赖性（Long Distance Dependency）“，n元模型将很难解析。<br>（n元模型的复杂度为$O(|V|^n)$，$V$是词汇量）</p>
</blockquote>
<p>尽管联想稀少，但对于人工处理来说仍然是天文数字级的。（联想视界论）</p>
</blockquote>
<p>接下来只需要进行文本统计，确定各个概率就行了。</p>
<hr>
<p>为了解决统计样本不足时的概率估计问题，诞生了很多<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E5%B9%B3%E6%BB%91">平滑</a>性的处理。</p>
<p>比如，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E5%9B%BE%E7%81%B5%E4%BC%B0%E8%AE%A1"><strong>古德-图灵估计</strong></a>（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/quicmous/article/details/52160940">More</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.shuang0420.com/2017/03/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95%28Smoothing%29%E5%B0%8F%E7%BB%93/">More2</a>）：从已有事件中统一分配一部分的概率给未知事件。</p>
<script type="math/tex; mode=display">
d_r=\cfrac{(r+1)N_{r+1}}{N_r}</script><p>公式表述了Good-Turing估计的核心。其中，$N_r$指语料库中频率为$r$的词数。</p>
<p>$r$较大时，</p>
<ul>
<li>统计中频率$\approx$概率。</li>
</ul>
<p>$r$较小时，我们利用Good-Turing估计，将$r$较小的词的概率进行<strong>修正</strong>。</p>
<ul>
<li>一般$N_r$和$r$的关系呈现<strong>凹性负相关</strong>（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E9%BD%8A%E5%A4%AB%E5%AE%9A%E5%BE%8B">Zipf定律</a>）<br>因而<strong>$r$ 越小，$N_r$上升越快</strong>，故我们修正得到的$d_r$会比$r$小得多。（并且，有正定性：$d_0&gt;0$）<br>这样，我们实际上只把一些中等频率的词的概率分配给了生僻词。（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Katz%27s_back-off_model">卡茨退避法</a>）</li>
</ul>
<blockquote>
<p>为了更直观的理解，我们还可以用反证法。<br>假如我们把前面的公式写为</p>
<script type="math/tex; mode=display">
r=\cfrac{(r+1)N_{r+1}}{N_r}（这实际并不成立）</script><p>可以得到</p>
<script type="math/tex; mode=display">
\cfrac{r}{r+1}=\cfrac{N_{r+1}}{N_r}</script><blockquote>
<p>对于$\cfrac{r}{r+1}$，在$r\rightarrow0$时，极限趋于1。</p>
<p>而对于$\cfrac{N_{r+1}}{N_r}$，由于$N_r$和$r$<strong>凹性负相关</strong>，因此在$r\rightarrow0$时，极限趋于$\infty$。（仅负相关则不能推出）</p>
</blockquote>
<p>故原假设不成立。并且，$r\rightarrow0$时，$0&lt;d_r≪r$。</p>
</blockquote>
<p>接下来只需要进行语料收集，训练模型就行了。</p>
<h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><h4 id="贪心法"><a href="#贪心法" class="headerlink" title="贪心法"></a>贪心法</h4><p>查字典，单向遍历字符串，每次贪心地找到最长的（字典里有的）匹配。（梁南元守创）</p>
<p>但是，贪心法对<strong>二义分割</strong>失效。</p>
<blockquote>
<p>例：发展中国家。</p>
<blockquote>
<p>字典：发，展，中，国，家，发展，中国，国家。</p>
</blockquote>
<p>从左往右：发展-中国-家。<br>从右往左：发展-中-国家。</p>
</blockquote>
<h4 id="动态规划-统计"><a href="#动态规划-统计" class="headerlink" title="动态规划+统计"></a>动态规划+统计</h4><p>由底至上地利用统计数据获得最大概率出现的分词方法。（拓展：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98">背包问题</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://raw.githubusercontent.com/tianyicui/pack/master/V2.pdf">背包九讲</a>）</p>
<blockquote>
<p>例：发展中国家。</p>
<blockquote>
<p>频率字典：发（0.05），展（0.01），中（0.02），国（0.04），家（0.03），发展（0.1），中国（0.2），国家（0.15），发展中（0.4）。</p>
</blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BB%E6%96%B9%E7%A8%8B">状态转移方程</a>可描述为：</p>
<script type="math/tex; mode=display">
dp[i,j]=max_{k\in\{i,i+1,…,j-1\}}(dp[i,k]+dp[k+1,j])</script><p>其中$dp[i,j]$指分词法在区间$[i,j]$的最大概率。（仅用加法粗糙地表示）</p>
<p>下为初始的频率字典表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">dp[i,j]</th>
<th style="text-align:center">j0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>i0</strong></td>
<td style="text-align:center">0.05</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">0.4</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.01</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>2</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>3</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.04</td>
<td style="text-align:center">0.15</td>
</tr>
<tr>
<td style="text-align:center"><strong>4</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.03</td>
</tr>
</tbody>
</table>
</div>
<p>经过状态转移方程（5）计算后得到：（仅利用了上三角。技巧：由底向上计算）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">dp[i,j]</th>
<th style="text-align:center">j0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>i0</strong></td>
<td style="text-align:center">0.05</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center"><strong>0.4</strong></td>
<td style="text-align:center">0.44</td>
<td style="text-align:center"><font color=red><strong>0.55</strong></font></td>
</tr>
<tr>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.01</td>
<td style="text-align:center">0.03</td>
<td style="text-align:center">0.21</td>
<td style="text-align:center">0.24</td>
</tr>
<tr>
<td style="text-align:center"><strong>2</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">0.2</td>
<td style="text-align:center">0.23</td>
</tr>
<tr>
<td style="text-align:center"><strong>3</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.04</td>
<td style="text-align:center"><strong>0.15</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>4</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.03</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>细节上可能还要实现回溯功能，dp[i,j]不为空时跳过……and so on</p>
</blockquote>
<p>更一般地，利用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95"><strong>Viterbi算法</strong></a>（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.zhihu.com/question/20136144">More</a>）。</p>
</blockquote>
<hr>
<p>介绍了颗粒度概念。</p>
<blockquote>
<p>机器翻译中大颗粒度更好；网页搜索中小颗粒度更好。</p>
</blockquote>
<h4 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h4><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/dearwind153/article/details/56009839">维特比算法</a>是针对<a target="_blank" rel="noopener external nofollow noreferrer" href="https://img-blog.csdn.net/20170220112902463">篱笆网络</a>（Lattice）的有向图最短路径提出的。（动态规划，<strong>传播式算法</strong>）</p>
<blockquote>
<p>凡是使用隐马尔可夫模型描述的问题都可以用它来解码。</p>
</blockquote>
<p>维特比算法的核心在于：<strong>传播</strong>。</p>
<hr>
<p>由于维特比算法的传播特性，维特比算法可以等效为一个<strong><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2">BFS</a></strong>。</p>
<p>如下图：（相当于一条马尔可夫链的状态伸展【上下伸展】，每一列代表原来的一个时刻）</p>
<img data-src="/Beauty-of-Mathmatics/U57DH6s.png" class="" title="篱笆图">
<blockquote>
<p>在两条竖线之间就是维特比算法的篱笆图。<br>维特比求的是最优值。其实就是求从左到右的最短路。<br>我们在左右端添加两个头尾节点（红色节点），连上一些0权的边（黄色边），即可以使用从节点$I$到$O$的BFS了。</p>
<p>设共有$N$列，每一列节点的最大数量记为$D$（篱笆高度），可以算出两列间边数的上限为$D^2$（篱笆间隙）。<br>维特比算法只须遍历所有的边，故算法复杂度为$O(ND^2)$。<br>（相比穷举的指数级复杂度：$O((D^2)^N)=O(D^N)$优化不少）</p>
</blockquote>
<p>总之，形象地说，BFS最短路径问题<strong>掐头去尾</strong>就得到了Viterbi算法。</p>
<p>由于任何的语音、文字的输入都是以流（Stream）的形式进行，只要处理每个状态的速度快过讲话、打字就行。此时，解码过程将是实时的。</p>
<hr>
<p>（我的思考）</p>
<p>纯粹幻想学中对幻想空间的纯化，就是一个逆向的BFS。</p>
<p>其次，为了结合BFS、DFS和统计学的优势，我构想了以下的算法。</p>
<p><strong>有限传播算法</strong>/有限优先算法（Finite First Search，FFS）：</p>
<ul>
<li>BFS只能无偏地传播，搜索进度慢，形成比较均衡的搜索树</li>
<li>DFS只能定向传播（又称为<strong>传递</strong>，因为仿佛有一个虚拟的信物/令牌在节点之间传递），容易陷入过于细分的领域，运气成分高，并且容易形成强烈失衡的搜索树</li>
<li>FFS综合了BFS和DFS，充分利用已知信息（通常信息集中于前几层），引入概率模型，在每一次搜索时选择概率最大的多个进行搜索，实现适度均衡的搜索树。（与<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/u010700335/article/details/44079069">剪枝</a>策略相反）</li>
<li>类似思想：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.zhihu.com/question/264189719">随机梯度下降</a>（Stochastic gradient descent，SGD）。<br>（在有限传播中可能找不到类似梯度这样局部的全面、精准的度量。有限传播算法考虑在有限资源的前提最大可能程度地获得答案）</li>
<li>类似算法：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/v_JULY_v/article/details/6093380">A*算法</a>。（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.gamedev.net/articles/programming/artificial-intelligence/a-pathfinding-for-beginners-r2003/">More</a>，启发式搜索算法）</li>
</ul>
<hr>
<p>维特比本人还参与将<a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/%E7%A0%81%E5%88%86%E5%A4%9A%E5%9D%80/2503754?fromtitle=CDMA&amp;fromid=185961">CDMA技术</a>（码分多址）应用于3G移动通信（More，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E9%AB%98%E9%80%9A">高通公司</a>）。</p>
<ul>
<li>频分多址（FDMA）：将不同的频率（频道）一一对应地分给用户。</li>
<li>时分多址（TDMA）：类似<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/u013630349/article/details/47732731">时间片轮转</a>，将同一频道切分为细小的时间片给不同用户。当切分得足够小，可近似具有实时性。</li>
<li>码分多址（CDMA）：类似<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E5%8F%A0%E5%8A%A0%E6%80%81">量子态叠加</a>（或者<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E5%82%85%E9%87%8C%E5%8F%B6%E5%88%86%E6%9E%90">Fourier解析</a>），用户通过一个密匙从大量的信息中过滤出自己的信号。</li>
</ul>
<blockquote>
<p>注：可以认为，址就是用户，<strong>分多址就是分给多个用户</strong>（使用）。</p>
</blockquote>
<h3 id="隐马尔可夫"><a href="#隐马尔可夫" class="headerlink" title="隐马尔可夫"></a>隐马尔可夫</h3><p>上文已经提到过通信模型：$【发送】信息1 \rightarrow编码\rightarrow信息2（信道）\rightarrow解码\rightarrow信息3【接收】$。</p>
<p>具体的，我们认为信息3和信息1是<strong>相似</strong>的。为了求取最佳信息3（<strong>解码</strong>），通过最大概然法。</p>
<p>假设信息1的序列：$S_1,S_2,…S_n$；信息2的序列：$O_1,O_2,…O_n$；信息3的序列：$s_1,s_2,…s_n$。</p>
<blockquote>
<p>假定我们已经拥有信息1的所有可能序列（样本）。</p>
</blockquote>
<p>利用相似原理和贝叶斯公式有</p>
<script type="math/tex; mode=display">
P(s_1,s_2,…s_n)=\frac{P(O_1,O_2,…O_n|S_1,S_2,…S_n)\cdot P(S_1,S_2,…S_n)}{P(O_1,O_2,…O_n)}</script><p>我们要<strong>求左式$P(s_1,s_2,…s_n)$的最大值</strong>，而$P(O_1,O_2,…O_n)$是固定的，不影响最值求解。因而我们可以简化为</p>
<script type="math/tex; mode=display">
P(s_1,s_2,…s_n)=c \cdot \{P(O_1,O_2,…O_n|S_1,S_2,…S_n)\cdot P(S_1,S_2,…S_n)\}</script><hr>
<p>最后，我们使用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B">隐含马尔可夫模型</a>来对上式进行估计。</p>
<p>马尔可夫链之前已经介绍过了。我们可以将其形成过程形象地称为“<strong>概率图灵机</strong>”。</p>
<blockquote>
<p>图灵机按照一定的规则在纸带上的状态上行走；概率图灵机将每一步固定的路线选择解析成有概率的事件。</p>
<p>记录这些事件发生概率的总表称为“<strong>概率转移矩阵</strong>”。（实质就是<strong>带权、归一</strong>的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5">邻接矩阵</a>）</p>
</blockquote>
<p>通过概率图灵机产生一个可观测的马尔可夫链：$S_1,S_2,…S_T$。</p>
<blockquote>
<p>显然，我们可以通过统计频率的方法来估计概率转移矩阵。</p>
</blockquote>
<p>但是，当观测条件受限时，则要引入隐马尔可夫。（可以想成<strong>唯象化</strong>的马尔可夫链）</p>
<p><strong>隐马尔可夫</strong>：<font color=red><strong>源</strong></font><strong>马尔可夫链</strong>（$S_1,S_2,…S_T$，隐序列）不可观测。我们将接收到一个新序列（$O_1,O_2,…O_n$，表序列），其中满足$O_i=f(S_i)$。</p>
<blockquote>
<p>注意，$O_i=f(S_i)$表示$O_i$是且仅是$S_i$的函数。（<strong>独立输出假设</strong>）</p>
</blockquote>
<p>如下图所示：</p>
<img data-src="/Beauty-of-Mathmatics/jW212Sg.png" class="" title="表序列和隐序列">
<p>计算特定S序列产生O序列的概率：</p>
<script type="math/tex; mode=display">
P(S_1,S_2,…S_n,O_1,O_2,…O_n)=\prod_tP(S_t|S_{t-1})\cdot P(O_t|S_t)</script><p>式右边利用了马尔可夫假设【$P(S<em>t|S</em>{t-1})$】和独立输出假设【$P(O_t|S_t)$】。</p>
<p>现在我们利用上式和前面的简化式的相似性完成对简化式的估计。</p>
<blockquote>
<p>相似性体现如下：</p>
</blockquote>
<script type="math/tex; mode=display">
翻译模型：P(O_1,O_2,…O_n|S_1,S_2,…S_n)=\prod_tP(O_t|S_t）</script><script type="math/tex; mode=display">
语言模型：P(S_1,S_2,…S_n)=\prod_tP(S_t|S_{t-1})</script><p>至此，解码问题将能利用隐马尔可夫模型解决。</p>
<hr>
<p>隐马尔可夫模型（<strong>HMM</strong>）有三个典型问题：</p>
<ul>
<li>（<font color=red><strong>预测</strong></font>）给定模型，计算特定O序列的概率【<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/allanjie/article/details/17884965">Forward-Backward算法</a>】</li>
<li>（<font color=red><strong>溯源</strong></font>）给定模型+特定O序列，估计最佳似然S序列【<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/218c1e4f0891">Viterbi算法</a>】</li>
<li>（<font color=red><strong>训练</strong></font>）<strong>给定观测数据，训练隐马尔可夫模型参数</strong>【<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/u014688145/article/details/53046765">Baum-Welch算法</a>】</li>
</ul>
<p>对于训练，可以使用监督学习获得参数$P(S<em>t|S</em>{t-1})和P(O_t|S_t)$，但标注数据的获取成本高。</p>
<blockquote>
<p>于是有了无监督的Baum-Welch算法。</p>
</blockquote>
<p>同样的O序列可能对应多个HMM模型（记为$M_{i}$）。</p>
<p>Baum-Welch算法将试图找到其中最可能的模型$M_{\theta}$（最佳模型）。</p>
<p>Baum-Welch算法（<strong>迭代</strong>）：</p>
<ul>
<li>首先找到任意一个满足O序列的HMM模型$M_i$</li>
<li>通过$M_1$可以利用O序列<strong>反向生成</strong>一个标注数据集</li>
<li>利用标注数据集，根据隐马尔可夫（解码算法）<strong>生成</strong>新的模型$M_{i+1}$</li>
<li>再次反向生成、生成……</li>
<li>……</li>
<li>直到收敛。找到质量足够好的模型$M_n$（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95">期望最大化</a>）<ul>
<li>用$M<em>n近似表示最佳模型M</em>{\theta}$。</li>
</ul>
</li>
</ul>
<blockquote>
<p>注意，Baum-Welch算法证明了$P（O|M_{i+1}）&gt;P（O|M_i）$，即，每次迭代都更优。</p>
</blockquote>
<p>由于一般得到的是局部最优（极值），Baum-Welch算法生成的模型可能稍逊于监督学习模型。</p>
<blockquote>
<p>对于单峰的凸函数（如，信息熵），则将表现正常。</p>
</blockquote>
<p>若为了确保得到全局最优，可以考虑结合<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95">遗传算法</a>。</p>
<h1 id="信息学"><a href="#信息学" class="headerlink" title="信息学"></a>信息学</h1><h2 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E5%85%8B%E5%8A%B3%E5%BE%B7%C2%B7%E9%A6%99%E5%86%9C">香农</a>创立的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E8%AE%BA">信息论</a>的基础内容不再介绍。只记录几个概念。</p>
<ul>
<li><p><strong>信息熵</strong>：$H(X)=-\sum_{x\in X}P(x)logP(x)$。</p>
</li>
<li><p><strong>条件熵</strong>：$H(X|Y)=-\sum_{x\in X,y\in Y}P(x,y)logP(x|y)$。</p>
</li>
</ul>
<blockquote>
<p>$H(X)\geq H(X|Y)$。（可以推广。意味着n元模型的精确度高于n-1元模型）</p>
</blockquote>
<ul>
<li><font color=red><strong>互信息</strong></font>：$I(X;Y)=\sum_{x\in X,y\in Y}P(x,y)log \cfrac{P(x,y)}{P(x)P(y)}$。</li>
</ul>
<blockquote>
<p>$I(X;Y)=H(X)-H(X|Y)$。</p>
</blockquote>
<ul>
<li><strong>交叉熵</strong>（相对熵）：$KL(f(x)\mid \mid g(x))=\sum_{x\in X}f(x)\cdot log\cfrac{f(x)}{g(x)}$。</li>
</ul>
<blockquote>
<p>交叉熵衡量<strong>差异度</strong>。差异越大，交叉熵越大。<br>交叉熵没有对称性。</p>
<p>为了获得对称性，提出了平均交叉熵：$JS(f(x)\mid\mid g(x))=\frac{1}{2}[KL(f(x)\mid \mid g(x))+KL(g(x)\mid \mid f(x))]$</p>
<p>应用：信号处理，同义词判断，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/index20001/article/details/78884646">语言模型复杂度</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/Tf-idf">TF-IDF</a>，<del>似乎可以用来查重orz</del>。</p>
</blockquote>
<hr>
<p>内容学习参见：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.xuetangx.com/courses/course-v1:TsinghuaX+70230063X+sp/about">学堂在线-《应用信息论基础》</a>。</p>
<p>指导读物：<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.ituring.com.cn/book/download/00aab03a-451d-43a2-b4ba-ee0483f27529">通信的数学理论</a>。（香农所著的信息论奠基性论文）</p>
<p>通俗读物：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://jackmk.ctfile.com/fs/14155983-203561084">信息简史</a>。（英文版）</p>
<p>教材参考书如下，[2]和[3]可作为延伸阅读材料：</p>
<blockquote>
<p>[1] Thomas M. Cover与Joy A. Thomas著的无比经典的教材《Elements of Information Theory》（中文版书名为《信息论基础》，阮吉寿、张华等译）；<br>[2] Abbas El Gamal和Young-Han Kim合著的《Network Information Theory》（中文版书名为《网络信息论》，张林译）；<br>[3] Imre Csiszár和János Körner Csizar所著的学院派经典《Information Theory: Coding Theorems for Discrete Memoryless Systems》。</p>
</blockquote>
<h2 id="信息指纹"><a href="#信息指纹" class="headerlink" title="信息指纹"></a>信息指纹</h2><h3 id="爬虫应用"><a href="#爬虫应用" class="headerlink" title="爬虫应用"></a>爬虫应用</h3><p>爬取过的URL的直接存储，非常占用内存。</p>
<p>但通过一个散列函数，将长长的URL映射到一个128位（16字节）的随机数，将能有效地控制空间。</p>
<blockquote>
<p>关键算法：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/dukai392/article/details/71155740">伪随机数产生器算法</a>（Pseudo-Random Number Generator，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/zmazon/article/details/17383521">PRNG</a>）。</p>
<p>现在常用的PRNG算法是<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E6%A2%85%E6%A3%AE%E6%97%8B%E8%BD%AC%E7%AE%97%E6%B3%95">梅森旋转算法</a>（Mersenne Twister）。</p>
</blockquote>
<p>这个随机数就称为URL的信息指纹。</p>
<h3 id="集合判等"><a href="#集合判等" class="headerlink" title="集合判等"></a>集合判等</h3><ul>
<li>暴力法：两两做比较，$O(N^2)$</li>
<li>排序法：先排序，再从头至尾比较，$O(NlogN)$</li>
<li>散列法：将第一个集合放到散列表中，然后拿第二个集合作对比，$O(N)$。但需要额外的$O(N)$空间。</li>
<li>指纹法：对集合$S$，定义指纹$FP(S)=FP(e_1)+FP(e_2)+…+FP(e_n)$。比较即可。就地算法。</li>
</ul>
<hr>
<p>如果集合只是相似。</p>
<p>可以按某些规则（比如，尾号是24的电子邮件地址）随机从集合中抽取几个元素，如果两个集合的抽查指纹相同，那么它们是相似的。</p>
<p>也可以使用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/Androidlushangderen/article/details/47134393">相似哈希</a>。</p>
<h4 id="反盗版（视频）"><a href="#反盗版（视频）" class="headerlink" title="反盗版（视频）"></a>反盗版（视频）</h4><p> 视频动辄上M、G的体量，几乎不可能采用普通的比较策略。</p>
<p>视频匹配的两个核心技术：</p>
<ul>
<li>关键帧提取<ul>
<li>尽管视频每秒可能有几十帧，但每一帧的差异不大。一般来说，每数秒才能提取出一个关键帧。</li>
</ul>
</li>
<li>特征提取<ul>
<li>利用信息指纹来表示关键帧，退化为集合判等问题</li>
</ul>
</li>
</ul>
<h4 id="指纹重复"><a href="#指纹重复" class="headerlink" title="指纹重复"></a>指纹重复</h4><p>信息指纹客观地存在着极小概率的重复可能性。</p>
<p>假设随机数范围是$0~N-1$，共$N$个。可以推出$k$个指纹不发生重复的概率：</p>
<script type="math/tex; mode=display">
P_k=\frac{(N-1)(N-2)…(N-k+1)}{N^{k-1}}</script><hr>
<p>（我的解法）</p>
<p>对上式，当N很大时，分子可以用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E6%96%AF%E7%89%B9%E6%9E%97%E5%85%AC%E5%BC%8F">斯特林公式</a>估计：$n!\approx \sqrt{2\pi n}(\frac{n}{e})^n$。</p>
<p>即：$(N-1)(N-2)…(N-k+1)=\cfrac{(N-1)!}{(N-k)!}\approx \cfrac{ \sqrt{2\pi (N-1)}(\frac{N-1}{e})^{N-1}}{ \sqrt{2\pi (N-k)}(\cfrac{N-k}{e})^{N-k}}$。</p>
<p>若我们有$k\ll N$（这一般是成立的，比如128位的指纹，$N=2^{128}$和$k\approx 10^{6}\approx 2^{20}$【百万级】），则可以进一步化简：（$c=e^{-(k-1)}$是一个关于$k$的小常数）</p>
<script type="math/tex; mode=display">
(N-1)(N-2)…(N-k+1)\approx \cfrac{ \sqrt{2\pi (N-1)}(\frac{N-1}{e})^{N-1}}{ \sqrt{2\pi (N-k)}(\cfrac{N-k}{e})^{N-k}}\approx  \cfrac{ (\cfrac{N-1}{e})^{N-1}}{ (\cfrac{N-k}{e})^{N-k}}\approx   c({N})^{k-1}</script><p>故我们得到原来的概率变为：</p>
<script type="math/tex; mode=display">
P_k=\frac{(N-1)(N-2)…(N-k+1)}{N^{k-1}}\approx \frac{c({N})^{k-1}}{N^{k-1}}=c=e^{-(k-1)}</script><p>即，当给定$N\gg 0$和$k\ll N$时，不冲突的概率$P_k$是与$k$强相关的指数衰减函数。（当然，数值上可能并不准确）</p>
<hr>
<p>在《数学之美》中采用的估计函数如下：</p>
<script type="math/tex; mode=display">
P_k=\frac{(N-1)(N-2)…(N-k+1)}{N^{k-1}}\approx e^{-\frac{1}{n}}e^{-\frac{2}{n}}…e^{-\frac{k}{n}}=exp(-\frac {k(k+1)}{2N})</script><p>同时，若要$k$个指纹重复的数学期望超过1，则$P_k<0.5$，此时可以解得$k>\frac {-1+\sqrt{1+8Nlog2}}{2}\approx2^{64}\approx 1.8\times 10^{19}$。可能性几乎为0。</p>
<h3 id="相似哈希"><a href="#相似哈希" class="headerlink" title="相似哈希"></a>相似哈希</h3><p>相似哈希是一种特殊的信息指纹。</p>
<p>假设一个网页有若干词$T：t_1,t_2,…t_k$，对应的权重（比如TF-IDF值）为$w_1,w_2,…_k$。计算出$T$向量的信息指纹$FP(T)$。（假设指纹为8位）</p>
<ul>
<li><p><strong>扩展</strong>【将8位指纹处理为8个实数$r_i$】</p>
<ul>
<li>```c++<br>for(int i=1;i&lt;=k;++i)<pre><code>r[i] = 0;  //初始化为0
</code></pre>for(int i=1;i&lt;=k;++i)  //这里在变量类型上是扩展的，但在信息上却是压缩的<pre><code>for(int j=1;j&lt;=8;++j)
    if(t[i][j]) r[j] += w[i];  //t[i][j]是词t_i的第j位
    else r[j] -= w[i];
</code></pre><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    经过上面的处理，我们将获得一个实数向量$R=\&#123;r_i\&#125;$。</span><br><span class="line"></span><br><span class="line">- **收缩**【将8个实数$r_i$重新布尔化为8位指纹】</span><br><span class="line"></span><br><span class="line">  - ```C++</span><br><span class="line">    bool P[9];</span><br><span class="line">    for(int i=1;i&lt;=8;++i)  //进一步压缩信息</span><br><span class="line">        P[i]=(bool)(r[i]);  //P向量是文章的信息指纹</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>我们获得了可以进行相似性比较的信息指纹$P$（相似Hash）。<br>若两个网站雷同，则相似哈希接近。</p>
</li>
</ul>
<h2 id="布隆过滤器"><a href="#布隆过滤器" class="headerlink" title="布隆过滤器"></a>布隆过滤器</h2><p>需求：<strong>判断一个元素是否在一个集合内</strong>。（联系之前的信息指纹部分——集合判等）</p>
<p>一般的散列表需要较多的容量才能存下集合中所有的元素。</p>
<p>布隆过滤器（Bloom Filter）仅需要散列表1/8到1/4的大小就能解决同样的问题。<br>它实际上是一个很长的二进制向量和一系列随机映射函数。</p>
<blockquote>
<p>例：假定存储1亿个电子邮件地址，先建立一个16亿bit的向量并清零。</p>
<ul>
<li>对每一个邮件地址$X$，用8个<strong>随机</strong>数产生器$F_i$先生成8个信息指纹$f_i$</li>
<li>然后将$f_i$用<strong>随机</strong>数产生器$G$将$f_i$映射到1~16亿的8个自然数$g_i$</li>
<li>将$g_i$对应的bit位设置为1</li>
<li>对1亿个电子邮件地址做相同处理</li>
</ul>
<p>这样，一个针对这些电子邮件地址的布隆过滤器就建好了。</p>
<p>检测可疑电子邮件：</p>
<ul>
<li>用$F_i$转换该地址，生成信息指纹$s_i$</li>
<li>将$s_i$用$G$映射到8个bit位，若对应为1，则命中</li>
</ul>
</blockquote>
<p>布隆过滤器能保证所有的命中，但可能会<strong>过度命中</strong>（一些不在集合中的元素也有极小的可能被命中）。</p>
<blockquote>
<p>补救：维护一个小的白名单。</p>
</blockquote>
<h2 id="密码学"><a href="#密码学" class="headerlink" title="密码学"></a>密码学</h2><p>密码学的道：</p>
<ul>
<li>无论获取多少密文，也无法消除己方情报系统的不确定性。<br>为了这个目的，不仅要<strong>密文之间相互无关</strong>，同时密文还是<strong>看似完全随机的序列</strong>。</li>
</ul>
<blockquote>
<p>注：根据信息守恒，当密文的形式越混沌，其可以携带的信息量就越少，则为了保证信息不失真，必然会在密钥（解密方法）上集聚极高的信息量。这值得权衡。</p>
</blockquote>
<h1 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h1><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p><strong>搜索之道</strong>：下载、索引和排序三大步骤。</p>
<hr>
<p>介绍<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E5%B8%83%E5%B0%94%E4%BB%A3%E6%95%B0">布尔代数</a>。</p>
<p><strong>真值表示法</strong>：用一个2进制真值串表示关键字。（由于大量<code>0</code>，属于稀疏，可改用字典【关键字-真值位置】结构）</p>
<p>假设互联网上约$10^{10}$个有意义页面，词汇表大小约$3\times 10^5$，压缩比约$100:1$。则最终索引的大小约$3\times10^{13}$。</p>
<blockquote>
<p>为了排名方便，还存有词频、词的位置等。<br>由于存储量巨大，故一般要采用<strong>分布式系统</strong>。</p>
</blockquote>
<h2 id="网络爬虫"><a href="#网络爬虫" class="headerlink" title="网络爬虫"></a>网络爬虫</h2><p>根据图论的两种典型遍历方法：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2">BFS</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2">DFS</a>。</p>
<p>互联网通过<a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/%E8%B6%85%E9%93%BE%E6%8E%A5">超链接</a>（有向边）的形式连成虚拟之海。</p>
<blockquote>
<p>注：可以查看我的文章了解更具体的技术：<a target="_blank" rel="noopener external nofollow noreferrer" href="http://skvel.tk/Pyspider%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97/">Pyspider操作指南</a>。</p>
</blockquote>
<h3 id="网络爬虫的构建"><a href="#网络爬虫的构建" class="headerlink" title="网络爬虫的构建"></a>网络爬虫的构建</h3><ol>
<li><strong>BFS或DFS</strong>？<ol>
<li>小容量爬虫，爬取首页即可，用BFS</li>
<li>考虑<a target="_blank" rel="noopener external nofollow noreferrer" href="https://baike.baidu.com/item/%E6%8F%A1%E6%89%8B/5800282">握手</a>（和网站建立连接）成本，在特定的网站适当采用DFS</li>
</ol>
</li>
<li><strong>页面分析和URL（链接）提取</strong><ol>
<li>通常采用队列，直接提取HTML中的URL标签</li>
<li>若网站采用了不规范的脚本（如JaveScript），需要通过高级模拟手段，利用浏览器内核解析网页</li>
</ol>
</li>
<li><strong>URL记录表</strong><ol>
<li>用一个散列表记录已经遍历过的网页（分摊复杂度$O(1)$）</li>
<li>分布式系统散列表的维护和访问是难题</li>
</ol>
</li>
</ol>
<h2 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h2><h3 id="质量"><a href="#质量" class="headerlink" title="质量"></a>质量</h3><p>对于特定查询，搜索结果的排名取决于两组信息：质量（Quality）信息和相关性（Relevance）信息。</p>
<p>早期搜索引擎的挫折：（按解决时间排序）</p>
<ul>
<li>收录网页少，只能对常见词索引</li>
<li>查询的结果相关性差</li>
</ul>
<p><strong>信任法则</strong>：指向一个网页的其它网页数量（节点的入度）越高，排名越高。（民主表决）</p>
<blockquote>
<p>为了更加合理地刻画高信任高权重的相关性，采用了权重。<br>即，一些被链接更多的网页，链接其它网页将具有更高的影响力。（可信度）</p>
</blockquote>
<p><strong>迭代原理</strong>：给定所有网页一个相同的初始排名，利用信任法则进行迭代计算，最终排名将收敛到真实值。</p>
<blockquote>
<p>由于互联网的链接是稀疏的，因此可以采用稀疏矩阵计算方法简化算法复杂度。<br>由于PageeRank更新一次很慢，因此诞生了并行计算方法（MapReduce）。<br>如果进一步考虑用户的点击数据，则可以进一步优化排名。</p>
<p>PageRank是一种<strong>整体化思维</strong>。</p>
</blockquote>
<p>对于PageRank，就是典型的联想图，如果一个幻想所链接的其它幻想越多，说明它更常见、更重要。但PageRank仍然有其局限所在，幻想的唯象程度太高了。</p>
<p>而且，由于是民主表决，PageRank将具有群体惯性，即，有偏向性。</p>
<p>搜索的目的是了解更广阔的世界，如果一个搜索引擎是有偏的，想界的扩张自然也就受限了。此时，用户更容易陷入神经制剂的纷扰当中。</p>
<p>另一方面，由于人类获取信息的能力有限，<strong>搜索引擎可能要在无偏和高效之前作出权衡</strong>。（联想公平和效率）</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/hguisu/article/details/7996185">PageRank的计算方法</a>。</p>
<p>简单示意如下：</p>
<script type="math/tex; mode=display">
A=\left[
\begin{matrix}
 a_{11} &\cdots     &a_{1j} &\cdots &a_{1N}      \\
 \vdots &\cdots& \vdots & \ddots & \vdots \\
 a_{i1}      &\cdots&a_{ij}      & \cdots &a_{iN}      \\
 \vdots &\cdots& \vdots & \ddots & \vdots \\
 a_{M1}      &\cdots&a_{Mj}      & \cdots &a_{MN}      \\
\end{matrix}
\right]
，B=\left[
\begin{matrix}
 b_1\\
 b_2\\
 \vdots\\
 \vdots\\
 b_N\\
\end{matrix}
\right]\\</script><p>其中，$A$为带权邻接矩阵（刻画网页链接），$B$为排名向量。</p>
<p>设初始排名为$B_0=[\frac{1}{N},\frac{1}{N},…,\frac{1}{N}]^T$，迭代方程如下：</p>
<script type="math/tex; mode=display">
B_{i+1}=A\cdot B_i</script><p>一般迭代10次左右就基本收敛。</p>
<h4 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h4><p>上述问题已经抽象为了一个矩阵相乘的问题。</p>
<p>若要充分利用分布式系统的优势，可以采用<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/u011983557/article/details/51213640">矩阵的分治法</a>。</p>
<p>另一种非严格意义上的分治是求结果矩阵的某一个元素。可以分出原两个矩阵中的对应行、列，然后再细分，将计算任务平均分配（此时分配是容易的），就能将计算时间缩短。这就是<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/MapReduce">MapReduce</a>的原理。（Map，分解任务；Reduce，整合结果）</p>
<h3 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h3><p>为了解决之前提到的PageRank可能具有的唯象性，引入了相关性的度量和排序。</p>
<p>影响搜索引擎质量的因素：</p>
<ul>
<li>用户的点击数据</li>
<li>完备的索引</li>
<li>网页质量的度量（PageRank）</li>
<li>用户偏好</li>
<li><strong>网页与查询的相关性</strong></li>
</ul>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p><strong>TF</strong>：Term Frequency，词频。</p>
<blockquote>
<p>一般采用相对词频：$\cfrac{关键词次数}{网页总字数}$。查询的所有关键词的词频相加，得到网页的相关性。<br>为了避免某些无意义的词的干扰（如“的”、“是”等），引入了<strong>停止词</strong>（这样的词不计入相关性统计）。<br>为了让一些更专业的词汇有更大的权重，引入了<strong>主题词</strong>（这样的词具有权重系数）。</p>
</blockquote>
<p>由TF得到了相关性的计算公式：</p>
<script type="math/tex; mode=display">
相关性=TF_1+TF_2+TF_3+…+TF_N</script><p>为了进一步确认权重，引入了IDF。</p>
<p><strong>IDF</strong>：Inverse Document Frequency，逆文本频率指数。</p>
<blockquote>
<p>定义式：$IDF=log\cfrac{D}{D_w}$。（关键词$w$在$D_w$个网页中出现过，$D$是总网页数）</p>
<p>假如中文网页数是$D=10亿$，停止词“的”在所有网页中出现，则它的$IDF=log\frac{10亿}{10亿}=0$。<br>假如专用词“原子能”在200万个网页中出现，那么它的$IDF=log\frac{10亿}{100万}=8.96$。（以2为底）<br>假如通用词“应用”在5亿个网页中出现，那么它的$IDF=log\frac{10亿}{5亿}=1$。</p>
</blockquote>
<p>于是，得到引入权重后的相关性：</p>
<script type="math/tex; mode=display">
相关性=TF_1\cdot IDF_1+TF_2\cdot IDF_2+TF_3\cdot IDF_3+…+TF_N\cdot IDF_N</script><hr>
<p>从本质上讲，$IDF$是一个特定条件下关键词的概率分布的交叉熵。</p>
<h3 id="权威性"><a href="#权威性" class="headerlink" title="权威性"></a>权威性</h3><p><strong>提及</strong>（Mention）：在文章段落中，讨论某个主题，<u>提及</u>了某个名称。</p>
<blockquote>
<p>当提及越多，则认为某个名称越权威。</p>
<p>提及隐藏在自然句中，需要自然语言处理方法。<br>即使有了好的算法，提及的计算仍然巨大。</p>
</blockquote>
<p>另一个难点是搜索结果的权威性的排序与搜索的主题相关。</p>
<p>假设有$M$个网页，$N$个关键词，则需要计算和存储$O(MN)$的结果。</p>
<blockquote>
<p>非常依赖于云计算和大数据技术。</p>
</blockquote>
<hr>
<p>计算权威度的步骤：</p>
<ul>
<li>对网页标题、正文进行句法分析，获取Mention信息</li>
<li>利用互信息，找到主题短语和信息源的相关性</li>
<li>对主题短语进行聚类（可用矩阵奇异值分解）</li>
<li>对网页进行聚类，权威性的度量只能建立在子域（Subdomain）或子目录（Subdirectory）等<strong>粗略</strong>的级上。</li>
</ul>
<h4 id="新闻分类"><a href="#新闻分类" class="headerlink" title="新闻分类"></a>新闻分类</h4><p>更广义地讲，文本分类的任务都是基于同一个原理。</p>
<p>假设词汇表有N个词。</p>
<p>我们统计新闻中出现的词，计算它们的TF-IDF值（没有出现过的为0），就可以形成一个关于该新闻的TF-IDF向量（N维）。</p>
<p>利用两个新闻的TF-IDF向量的夹角，可以来反映新闻之间的相似度。</p>
<blockquote>
<p>这里利用余弦定理即可。</p>
</blockquote>
<p>通过这个的方法，我们可以设定一个相似度阀值，将小于阀值的新闻归到一类。在分好的小类里，又可以继续求类与类之间的相似度，从而获得更大的分类……</p>
<hr>
<p>大数据时余弦计算的额外处理：</p>
<ul>
<li>充分利用TF-IDF向量中的非零元素</li>
<li>删除虚词/停止词（消除了噪声）</li>
<li>考虑词的位置加权（比如，在标题、首尾进行额外加权）</li>
</ul>
<h4 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h4><p>上文中的余弦算法只适合于处理中大规模的文本（百万级），对于超大规模文本（亿级），则需要相对快速、粗糙的算法。</p>
<p>之前的新闻分类本质上是一个聚类问题，但是需要每个向量两两做计算。我们希望有一个办法——<strong>一次性地计算出所有的相关性</strong>——矩阵<strong><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3">奇异值分解</a></strong>（Singular Value Decomposition，<strong>SVD</strong>）。</p>
<p>我们需要一个超大的矩阵$A_{M\times N}$来描述成千上万的文章和上百万的词的关联性。</p>
<p>奇异值分解：（$r≪M,r≪N$）</p>
<script type="math/tex; mode=display">
A_{M\times N}=X_{M\times r}\cdot B_{r\times r}\cdot Y_{r\times N}</script><p>$X_{M\times r}$：每一行（M）代表一个词，每一列（r）表示一个语义相近的词类。（元素值代表相关度）</p>
<p>$Y_{r\times N}$：每一列（N）代表一篇文本， 每一行（r）对应一个主题。</p>
<p>$B_{r\times r}$：表示词类和文章类（主题）的相关性。</p>
<p>只要对$A_{M\times N}$做一次奇异值分解，就可以同时完成近义词分类和文章分类，还能得到相应的相关性。</p>
<h2 id="地图定位"><a href="#地图定位" class="headerlink" title="地图定位"></a>地图定位</h2><p>智能手机的定位、导航：</p>
<ul>
<li>利用卫星定位</li>
<li>地址的识别</li>
<li>根据起点和终点，规划最短或最快路线</li>
</ul>
<h3 id="有限状态机"><a href="#有限状态机" class="headerlink" title="有限状态机"></a>有限状态机</h3><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/zh-hans/%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA">有限状态机</a>用于地址的识别。</p>
<p>简单来说，有限状态机能够在有向无环状态图中不可逆地转换，当转换失败时则地址无效。</p>
<p>为了让识别具有一定的容错性（输错了一点仍然能够查询），提出了基于概率的有限状态机。（与马尔可夫链基本等效）</p>
<p>其它应用：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.google.co.uk/landing/now/">Google Now</a>。</p>
<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>对于导航系统，一般会采用动态规划的办法。</p>
<p>从A点到B点的路线，一定会经过一个里程数不断增长的过程。也就是说，这是一个<strong>传播问题</strong>。因此，全局的最优，一定也是局部里程圈的最优。至此，形成了状态转移。</p>
<h3 id="有限状态传感器"><a href="#有限状态传感器" class="headerlink" title="有限状态传感器"></a>有限状态传感器</h3><p><strong>加权有限状态传感器</strong>：Weighted Finite State Transducer，<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.gavo.t.u-tokyo.ac.jp/~novakj/wfst-algorithms.pdf">WFST</a>。有限状态机中的每个状态由输入和输出符号定义。</p>
<blockquote>
<p>任何一个词的前后二元组，都可以对应到WFST的一个状态。</p>
<p>若在符号序列中某个时刻前后出现了形如$AB$的有序对。（相当于A是输入，B是输出）<br>则根据在有限状态机中对应的$…\stackrel{A}{\longrightarrow}  STATE_i\stackrel{B}{\longrightarrow} …$结构，进入对应$STATE_i$状态。</p>
<p>可用于<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/l_b_yuan/article/details/50876340">语音识别</a>。</p>
</blockquote>
<h2 id="SEO与作弊"><a href="#SEO与作弊" class="headerlink" title="SEO与作弊"></a>SEO与作弊</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://searchengineland.com/guide/what-is-seo">SEO</a>就是搜索引擎优化。</p>
<blockquote>
<p>SEO一般要适当，遵循搜索引擎的规则。</p>
<p>一个<u>无所不用其极</u>的SEO优化就称为在搜索引擎中的作弊。</p>
</blockquote>
<hr>
<p>早期作弊手法：</p>
<ul>
<li>重复关键词<ul>
<li>利用TF的相对词频</li>
</ul>
</li>
<li>买卖链接<ul>
<li>识别链接的流通</li>
<li>利用出链（Out Links）向量，通过余弦算法聚类，识别卖链接的网站</li>
</ul>
</li>
</ul>
<p>利用通信模型解决搜索反作弊：</p>
<ul>
<li>从信息源出发，加强通信（编码）自身的<strong>抗干扰</strong>能力<ul>
<li>利用相反的信号抵消噪音</li>
<li>搜集一段时间的作弊信息后，还原其原有的排名</li>
</ul>
</li>
<li>从传输来看，<strong>过滤掉噪音</strong>，还原信息</li>
</ul>
<p>利用图论：</p>
<ul>
<li>互相链接的节点称为Clique，发现Clique并直接应用到反作弊中</li>
</ul>
<p>利用浏览内核解析：</p>
<ul>
<li>对于使用JavaScrpt跳转页面，其落地页（Landing Page）质量非常高，但进入之后会立即通过JS程序被跳转到商业网站</li>
</ul>
<hr>
<p>反作弊是自动、无偏的。根本是<strong>去噪声</strong>。<br>一个网站的内容质量，决定其排名。<br>越流行的搜索引擎，SEO越风行。</p>
<h2 id="搜索广告"><a href="#搜索广告" class="headerlink" title="搜索广告"></a>搜索广告</h2><p>搜索广告的三个阶段：</p>
<ul>
<li>竞价排名（Overture、百度）</li>
<li>利用经验预估点击率（Click Through Rate，CTR），预测用户的点击概率</li>
<li>逻辑回归模型</li>
</ul>
<p>逻辑回归模型：将一个事件出现的概率逐渐适应到一条逻辑曲线（Logistic Curve）上。</p>
<blockquote>
<p>一个简单的逻辑回归函数：$f(z)=\cfrac{e^Z}{e^Z+1}=\cfrac{1}{1+e^{-Z}}$。（指数模型）</p>
</blockquote>
<img data-src="/Beauty-of-Mathmatics/Logistic-curve.svg" class="" title="逻辑回归">
<blockquote>
<p>逻辑回归的定义域在$(-\infty ,+\infty)$，值域在$(0,1)$。</p>
<p>对于$(-\infty ,+\infty)$，对于任何信号都可进行回归。<br>而对于$[0,1]$，可以看作概率函数，于是逻辑回归与概率分布联系起来。</p>
</blockquote>
<p>对于预估点击率而言，假设$k$个影响变量$x_1,x_2,…,x_k$。</p>
<p>则可线性组合得到$z=\beta_0+\beta_1x_1+\beta_2x_2+…+\beta_kx_k$。</p>
<p>${\beta_i}$向量就是该模型的参数向量。可以通过神经网络训练得到。比如最大熵模型的$GIS$或者$IIS$算法。</p>
<h1 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a>数学模型</h1><ul>
<li>一个正确的模型应当在形式上是简单的</li>
<li>一个正确的模型最开始可能并不如复杂的错误模型准确</li>
<li>大量的准确数据对研发很重要</li>
<li>正确的模型可能受到噪声干扰，此时应坚定信心，积极找寻噪声源</li>
</ul>
<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://wanghuaishi.wordpress.com/2017/02/21/%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%A4%A7%E7%86%B5%E5%8E%9F%E7%90%86%EF%BC%88the-maximum-entropy-principle%EF%BC%89/"><strong>最大熵原理</strong></a>（The Maximum Entropy Principle）：在已知信息的基础上，不做任何主观假设，使未知事件的预测信息熵最大。此时的概率分布称为最大熵模型。</p>
<blockquote>
<p>这里的熵是信息熵。</p>
<p>保留全部的不确定性，将风险降到最小。</p>
</blockquote>
<p>希萨证明：对任何一组不自相矛盾的信息，最大熵模型存在且唯一。（其形式是指数函数）</p>
<blockquote>
<p>例如：根据$w_1,w_2$预测$w_3$。</p>
<p>$P(w_3|w_1,w_2,s)=\cfrac{1}{Z(w_1,w_2,s)}e^{\lambda_1(w_1,w_2,w_3)+\lambda_2(s,w_3)}$.<br>（$Z$是<strong>归一</strong>化因子，保证$P$概率归一，与参数$\lambda_i$一起需要被训练出来）</p>
</blockquote>
<p>但是，<strong>最大熵模型的计算量相当大</strong>。</p>
<blockquote>
<p>比如，若搜索的排序需要考虑20种特征。</p>
<script type="math/tex; mode=display">
{x_1,x_2,…,x_{20}}</script><p>待排序的网页是$d$。</p>
<p>则有最大熵模型：</p>
<script type="math/tex; mode=display">
P(d|{x_1,x_2,…,x_{20}})=\cfrac{1}{Z({x_1,x_2,…,x_{20}})}e^{\lambda_1(x_1,d)+\lambda_1(x_2,d)+…+\lambda_1(x_{20},d)}</script><p>其中，归一化因子为</p>
<script type="math/tex; mode=display">
Z({x_1,x_2,…,x_{20}})=\sum^de^{\lambda_1(x_1,d)+\lambda_1(x_2,d)+…+\lambda_1(x_{20},d)}</script><p>（参数$\lambda_i$通过模型的训练得到）</p>
</blockquote>
<p>最原始的最大熵模型训练方法是<strong>通用迭代算法</strong>（Generalized Lterative Scaling，$GIS$）。</p>
<ul>
<li>假定第零次迭代的初始模型为等概率的均匀分布（类似PageRank的设定）</li>
<li>用第N次迭代的模型来估算每种信息特征在训练数据中的分布，比较并调节相应模型参数。</li>
<li>重复，直至收敛。</li>
</ul>
<blockquote>
<p>GIS算法是一个典型的期望值最大化算法（Expectation Maximization，EM）。</p>
<p>GIS收敛缓慢，而且不稳定，因此很少有人真的使用。</p>
</blockquote>
<p>后来出现了<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.cs.cmu.edu/~aberger/pdf/scaling.pdf">改进迭代算法</a>$IIS$（Improved Iterative Scaling），训练时间缩短了1至2个量级。</p>
<p>吴军提出了更快的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cs.jhu.edu/~junwu/publications.html">最大熵模型快速算法</a>，训练时间再次缩短了1至2个量级。</p>
<hr>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://money.jrj.com.cn/2017/07/07101822712242.shtml">文艺复兴公司</a>（Renaissance Technologies）：利用最大熵模型和其它一些先进的数学工具，成为了世界上最成功的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E5%AF%B9%E5%86%B2%E5%9F%BA%E9%87%91">对冲基金</a>公司。</p>
<blockquote>
<p>注意：更多相关知识可了解<a target="_blank" rel="noopener external nofollow noreferrer" href="https://wiki.mbalib.com/wiki/%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93">量化交易</a>。</p>
</blockquote>
<h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><p><strong>句法分析</strong>（Sentence Parsing）：</p>
<ul>
<li>一是指根据文法对一个句子进行分析，建立其语法树，即<strong>文法分析</strong>（Synatactic Parsing）；</li>
<li>二是指对一个句子中各成分的语义进行分析，得到对这个句子语义的一种描述（语义树），即<strong>语义分析</strong>（Semantic Parsing）。</li>
</ul>
<p>这里主要讨论文法分析。</p>
<hr>
<p>Eugene Charniack提出原则：选择文法规则，让被分析的句子的语法树概率达到最大。</p>
<p>拉纳帕提则进行了进一步的优化，真正将数学模型和文法分析结合起来。</p>
<ul>
<li>对任意一个句子进行分词<ul>
<li>美联储|主席|本·伯南克|昨天|告诉|媒体|7 000 亿|美元|的|救助|资金|将|借给|上百|家|银行|、|保险公司|和|汽车公司</li>
</ul>
</li>
<li>扫描（从左往右），整合出高一阶的词组<ul>
<li>（美联储主席）|本·伯南克|昨天|告诉|媒体|（7 000 亿美元）|的|（救助资金）|（将借给）|（上百家）|（银行、保险公司和汽车公司）</li>
</ul>
</li>
<li>重复扫描，整合……<ul>
<li>【美联储主席本·伯南克】|昨天|告诉|媒体|【7 000 亿美元的救助资金】|（将借给）|【上百家银行、保险公司和汽车公司】</li>
</ul>
</li>
<li>直到仅剩一个括号为止</li>
</ul>
<p>容易看出，每次扫描，句子成分数按一定比例缩减，因而这个方法的复杂度是$O(n)$线性的。</p>
<hr>
<p>上述方法对于模糊的语言处理能力较弱，为了处理更加随意的文法分析，逐渐形成了<strong>浅层分析</strong>（Shallow Parsing），找到句子中主要的词组已经它们对应的关系即可。</p>
<p><strong>条件随机场</strong>使得浅层分析的正确率大大提高。（栅栏图）</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/55755fc649b1">条件随机场</a>是隐马尔可夫模型的多元推广版本，此时观测序列的Oi将与前后的状态都相关，即与$S<em>{i-1},S_i,S</em>{i+1}$相关。（S序列仍然是马尔可夫链，但独立输出假设被推广了）</p>
<blockquote>
<p>条件随机场是无向图。仍然遵循马尔可夫假设。</p>
</blockquote>
<p>整个条件随机场的量化模型就是O序列和S序列的联合概率分布$P(O,S)$：</p>
<script type="math/tex; mode=display">
P(O,S)=P(O_1,O_2,…,O_n,S_1,S_2,…,S_m)</script><p>由于变量过多，不可能有足够的数据来估计这个高维分布。因此，一般要降维处理，通过一些边缘分布（如$P(O_1)$，$P(S_2)$，$P(O_1,S_3)$等）来找出一个符合这些条件的概率分布（通常不止一个满足）。</p>
<p>根据最大熵原理，希望找到一个满足所有边缘分布的熵最大的模型（指数模型）。</p>
<p>每一个边缘分布对应指数模型中的一个特征$f_i$（Feature）。（$f_i$的参数可以使用最大熵算法训练）</p>
<blockquote>
<p>比如：边缘分布$f_i(O_1,O_2,…,O_n,S_1,S_2,…,S_m)=f_i(x_1)$。</p>
</blockquote>
<p>将特征运用到模型之中，有</p>
<script type="math/tex; mode=display">
P(O_1,O_2,…,O_n,S_1,S_2,…,S_m)=\cfrac{e^{f_1+f_2+…+f_k}}{Z}</script><p>这时就可以进行浅层文法分析了。（例：Google的文法分析器<a target="_blank" rel="noopener external nofollow noreferrer" href="http://johng.cn/gf-gparser/">Gparser</a>）</p>
<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>马尔可夫链是状态序列，每个状态值取决于前面有限个状态。</p>
<p>在现实生活中，关系可能是复杂、错综的，形成一个有向图网络。</p>
<p>若在这个网络之中，每一个状态只跟与其直接相连的状态有关，则称为一个<strong>贝叶斯网络</strong>。</p>
<blockquote>
<p>贝叶斯网络是马尔可夫链的推广。</p>
</blockquote>
<p>使用贝叶斯网络必须确定网络的拓扑结构，和各种状态之间的转移概率。</p>
<blockquote>
<p>得到拓扑结构称为结构训练，得到转移概率称为参数训练。</p>
</blockquote>
<p>从理论上说，贝叶斯网络是一个NP完全问题（现有计算机不可计算）。</p>
<p>但对于某些应用，经过适当简化（如，贪心+蒙特卡洛，利用互信息简化网络等），训练也能被计算机完成。</p>
<blockquote>
<p>如Google的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.zhihu.com/question/22741652">Rephil</a>，通过贝叶斯网络完成对词、概念（词类）、文章的联系，将上千万关键词合成了上百万概念的聚类。</p>
</blockquote>
<h3 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h3><p>下图来自<a target="_blank" rel="noopener external nofollow noreferrer" href="https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53"><strong>Here</strong></a>，展示了一个典型的四层人工神经网络（多层感知机）的运作方式：</p>
<p><img data-src="https://cdn-images-1.medium.com/max/1600/1*eEKb2RxREV6-MtLz2DNWFQ.gif" alt="多层感知机"></p>
<p>可以认为<a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Artificial_neural_network">神经网络</a>是一种特殊的有向图（90度左旋后的篱笆图）。</p>
<ul>
<li>所有节点分层，每一层通过有向弧指向上一层节点，但同一层无弧连接，不跨层连接</li>
<li>每条弧有一个权重</li>
<li>完了</li>
</ul>
<blockquote>
<p><strong><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/dcrmg/article/details/73743742">神经元函数</a></strong>：为线性模型增加一次非线性变换，从而增强神经网络的分类能力。</p>
</blockquote>
<h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><p><strong><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">感知机</a></strong>：一个输出层，一个输入层。一个计算夹层。最简单的神经网络。</p>
<p><img data-src="https://cdn-images-1.medium.com/max/800/1*n6sJ4yZQzwKL9wnF5wnVNg.png" alt="感知机"></p>
<blockquote>
<p>详细内容直接参考<a target="_blank" rel="noopener external nofollow noreferrer" href="https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53">Here</a>吧。</p>
</blockquote>
<p>Minsky<a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Perceptrons_(book">指出</a>)：感知机无法解决异或问题。（<a target="_blank" rel="noopener external nofollow noreferrer" href="https://skymind.ai/wiki/multilayer-perceptron">More</a>）</p>
<h4 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h4><p>实际上神经网络的计算过程就是矩阵运算。相当于：</p>
<script type="math/tex; mode=display">
Vector_{output}=A_n\times A_{n-1}\times …\times A_1\times Vector_{inputr}</script><p>为了基于线性分类实现复杂的非线性分类，多层的神经网络采用复杂函数拟合，利用隐藏层完成非线性空间到线性的变换。</p>
<p>一般来说，层数越多，神经网络的效果越好。（如<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/subconscious/p/5058741.html#sixth">下图</a>）</p>
<img data-src="/Beauty-of-Mathmatics/lMn0aAi.jpg" class="" title="多层神经网络">
<hr>
<blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html">神经网络入门</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/subconscious/p/5058741.html">神经网络浅讲：从神经元到深度学习</a></p>
</blockquote>
<h2 id="期望最大化"><a href="#期望最大化" class="headerlink" title="期望最大化"></a>期望最大化</h2><blockquote>
<p>期望最大化算法（Expectation Maximization Algorithm，EM算法）是上帝的算法。</p>
</blockquote>
<p>前面已经介绍过两种文本分类算法：</p>
<ul>
<li>利用事先设定好的类别对新的文本进行分类<ul>
<li>需要事先设定好类别和文本中心（Centroids）</li>
</ul>
</li>
<li>自底向上地将文本两两比较进行聚类<ul>
<li>计算时间较长</li>
</ul>
</li>
</ul>
<p>基于EM算法，提出了一种新的迭代算法（无须事先设定，无须聚类）。</p>
<ul>
<li><strong>随机挑选类别的中心，然后反复优化直到收敛</strong><ul>
<li>要利用到文本TF-IDF向量和余弦算法</li>
</ul>
</li>
</ul>
<hr>
<p>记同一类中各个点到中心的平均距离为$d$，不同类中心之间的距离为$D$。</p>
<p><strong>我们希望每次迭代，$d$变小，$D$变大</strong>。</p>
<p>假设第1类到第K类中分别有$n_1,n_2,…,n_k$个点。每一类到中心的平均距离为$d_1,d_2,…,d_k$。</p>
<p>则有总平均距离</p>
<script type="math/tex; mode=display">
d=(n_1\times d_1+n_2\times d_2+…+n_k\times d_k)/k</script><p>同理</p>
<script type="math/tex; mode=display">
D=\sum_{i,j}\cfrac{D_{ij}}{k(k-1)}（D_{ij}为i类和j类的距离）</script><script type="math/tex; mode=display">
若考虑点的数量，则有D=\sum_{i,j}\cfrac{D_{ij}n_in_j}{n(n-1)}</script><p>假定有一个点$x$，它在前一次迭代中属于第$i$类，下一次迭代后被被安排到第$j$类。</p>
<p>不难<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/zouxy09/article/details/8537620">证明</a>，</p>
<script type="math/tex; mode=display">
d(i+1)<d(i)且D(i+1)>D(i)</script><hr>
<p>Baum-Welch算法（隐马尔可夫链），GIS算法（最大熵模型）都是典型的EM算法。</p>
<p>EM算法的分布描述：</p>
<ul>
<li>E过程（Expectation）：根据已有模型，计算观测数据输入到模型的结果<ul>
<li>Baum-Welch：计算每个状态转移的次数和输出的次数</li>
<li>GIS：计算每一个特征的数学期望值</li>
</ul>
</li>
<li>M过程（Maximization）：重新计算模型参数，以最大化期望<ul>
<li>Baum-Welch：根据这些次数重新估计隐含马尔可夫模型的参数</li>
<li>GIS：根据数学期望值和实际观测值的比，调整最大熵模型参数</li>
</ul>
</li>
</ul>
<p>EM算法优化的目标函数必须是一个凸函数，才能确保得到全局最优解。</p>
<hr>
<p>就到这里吧。</p>
<div><hr><font size=5>相关文章</font><ul><li><a href="https://stellarkey.github.io/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%AD%E7%9A%84%E6%96%B0%E6%95%B0%E5%AD%A6/">人工智能中的“新”数学</a></li><li><a href="https://stellarkey.github.io/CS%E6%8C%91%E6%88%98%E8%90%A5/">CS挑战营</a></li><li><a href="https://stellarkey.github.io/%E6%A5%94%E5%AD%90-%E6%83%B3%E8%B1%A1%E5%8A%9B/">楔子-想象力</a></li></ul></div>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>工云
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://stellarkey.github.io/Beauty-of-Mathmatics/" title="阅《数学之美》">https://stellarkey.github.io/Beauty-of-Mathmatics/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 数学</a>
              <a href="/tags/%E5%B9%BB%E6%83%B3%E5%AD%A6/" rel="tag"><i class="fa fa-tag"></i> 幻想学</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/m%C3%97n%E7%9F%A9%E9%98%B5%E7%9A%84%E8%A1%8C%E5%88%97%E5%BC%8F/" rel="prev" title="m×n矩阵的行列式">
      <i class="fa fa-chevron-left"></i> m×n矩阵的行列式
    </a></div>
      <div class="post-nav-item">
    <a href="/Intro-to-AI-Lecture/" rel="next" title="人工智能小讲座简述">
      人工智能小讲座简述 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">自然语言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%B7%E6%BA%90%E6%97%B6%E6%9C%9F"><span class="nav-number">1.1.</span> <span class="nav-text">起源时期</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E6%8A%98%E6%97%B6%E6%9C%9F"><span class="nav-number">1.2.</span> <span class="nav-text">转折时期</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">统计模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.1.</span> <span class="nav-text">通用模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E8%AF%8D"><span class="nav-number">1.3.2.</span> <span class="nav-text">分词</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%AA%E5%BF%83%E6%B3%95"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">贪心法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E7%BB%9F%E8%AE%A1"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">动态规划+统计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Viterbi%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">Viterbi算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB"><span class="nav-number">1.3.3.</span> <span class="nav-text">隐马尔可夫</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%AD%A6"><span class="nav-number">2.</span> <span class="nav-text">信息学</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA"><span class="nav-number">2.1.</span> <span class="nav-text">信息论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E6%8C%87%E7%BA%B9"><span class="nav-number">2.2.</span> <span class="nav-text">信息指纹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E5%BA%94%E7%94%A8"><span class="nav-number">2.2.1.</span> <span class="nav-text">爬虫应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E5%90%88%E5%88%A4%E7%AD%89"><span class="nav-number">2.2.2.</span> <span class="nav-text">集合判等</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E7%9B%97%E7%89%88%EF%BC%88%E8%A7%86%E9%A2%91%EF%BC%89"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">反盗版（视频）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E7%BA%B9%E9%87%8D%E5%A4%8D"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">指纹重复</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E4%BC%BC%E5%93%88%E5%B8%8C"><span class="nav-number">2.2.3.</span> <span class="nav-text">相似哈希</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8"><span class="nav-number">2.3.</span> <span class="nav-text">布隆过滤器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%86%E7%A0%81%E5%AD%A6"><span class="nav-number">2.4.</span> <span class="nav-text">密码学</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2"><span class="nav-number">3.</span> <span class="nav-text">搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B4%A2%E5%BC%95"><span class="nav-number">3.1.</span> <span class="nav-text">索引</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB"><span class="nav-number">3.2.</span> <span class="nav-text">网络爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E6%9E%84%E5%BB%BA"><span class="nav-number">3.2.1.</span> <span class="nav-text">网络爬虫的构建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PageRank"><span class="nav-number">3.3.</span> <span class="nav-text">PageRank</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%A8%E9%87%8F"><span class="nav-number">3.3.1.</span> <span class="nav-text">质量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MapReduce"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">MapReduce</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="nav-number">3.3.2.</span> <span class="nav-text">相关性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TF-IDF"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">TF-IDF</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9D%83%E5%A8%81%E6%80%A7"><span class="nav-number">3.3.3.</span> <span class="nav-text">权威性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%B0%E9%97%BB%E5%88%86%E7%B1%BB"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">新闻分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="nav-number">3.3.3.2.</span> <span class="nav-text">奇异值分解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%B0%E5%9B%BE%E5%AE%9A%E4%BD%8D"><span class="nav-number">3.4.</span> <span class="nav-text">地图定位</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA"><span class="nav-number">3.4.1.</span> <span class="nav-text">有限状态机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="nav-number">3.4.2.</span> <span class="nav-text">动态规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E4%BC%A0%E6%84%9F%E5%99%A8"><span class="nav-number">3.4.3.</span> <span class="nav-text">有限状态传感器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SEO%E4%B8%8E%E4%BD%9C%E5%BC%8A"><span class="nav-number">3.5.</span> <span class="nav-text">SEO与作弊</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E5%B9%BF%E5%91%8A"><span class="nav-number">3.6.</span> <span class="nav-text">搜索广告</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.</span> <span class="nav-text">数学模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">最大熵模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA"><span class="nav-number">4.2.</span> <span class="nav-text">条件随机场</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">4.3.</span> <span class="nav-text">贝叶斯网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.3.1.</span> <span class="nav-text">人工神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">4.3.1.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">4.3.1.2.</span> <span class="nav-text">多层感知机</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="nav-number">4.4.</span> <span class="nav-text">期望最大化</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="工云"
      src="/images/stellarkey.gif">
  <p class="site-author-name" itemprop="name">工云</p>
  <div class="site-description" itemprop="description">Never stop thinking.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/stellarkey" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;stellarkey" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:velocfc@gmail.com" title="E-Mail → mailto:velocfc@gmail.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fas fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" class="cc-opacity" rel="noopener external nofollow noreferrer" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      云海之上
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.iszy.cc/" title="https:&#x2F;&#x2F;www.iszy.cc&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">随遇而安</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://asdfv1929.github.io/" title="https:&#x2F;&#x2F;asdfv1929.github.io&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">asdfv1929</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://printempw.github.io/" title="https:&#x2F;&#x2F;printempw.github.io&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">PRIN BLOG</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.cfzhao.com/" title="http:&#x2F;&#x2F;www.cfzhao.com&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">ZHAOYUWEI</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hanherbert.github.io/" title="https:&#x2F;&#x2F;hanherbert.github.io&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">知青</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://louieworth.github.io/" title="https:&#x2F;&#x2F;louieworth.github.io&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">louieworth</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://hgen27.github.io/" title="https:&#x2F;&#x2F;hgen27.github.io&#x2F;" rel="noopener external nofollow noreferrer" target="_blank">Hgen</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-chart-line"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vel</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">538k</span>
</div>

        








      </div>
    </footer>
  </div>

  


  
  <style>
  
  button.darkmode-toggle 
{
  z-index: 9999;
  }
  
  img, .darkmode-ignore {
    isolation: isolate;
    display: block;
  }
  </style>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/lozad.js/1.14.0/lozad.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
  <script src="/lib/darkmode-js/lib/darkmode-js.min.js"></script>


<script>
var options = {
  bottom: '32px', // default: '32px'
  right: '64px', // default: '32px'
  left: 'unset', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: true // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
// window.onload = function(){
//   setTimeout(
//     function() {
//       document.getElementsByClassName('darkmode-toggle')[0].click();
//     },
//     550,
//   );
//   document.getElementsByClassName('darkmode-toggle')[0].click();
// }
</script>
<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : 27281,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//unpkg.com/quicklink@2.2.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://stellarkey.github.io/Beauty-of-Mathmatics/',]
      });
      });
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
